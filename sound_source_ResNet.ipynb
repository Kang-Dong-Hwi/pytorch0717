{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "sound_source_ResNet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BOn5xCyhP_c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "14496f7f-8235-49df-ad5a-2e61e593a117"
      },
      "source": [
        "\n",
        "import torch\n",
        "import os\n",
        "import random\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "import librosa\n",
        "import librosa.display\n",
        "import numba.decorators\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from numba.decorators import jit as optional_jit\n",
        "\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "\n",
        "#PATH = 'C://Projects//keras_talk//keras//intern//dataset//'\n",
        "PATH = '/content/gdrive/My Drive/dataset/'\n",
        "\n",
        "train_size = 800\n",
        "test_size = 200\n",
        "\n",
        "EPOCHS = 3\n",
        "BATCH_SIZE = 50\n",
        "\n",
        "\n",
        "def Y_DATA(y_data):\n",
        "    for idx in range(y_data.shape[0]):\n",
        "        y = y_data[idx]\n",
        "        if y < 0:  y_data[idx] = 10\n",
        "        else:      y_data[idx] = (y//20)\n",
        "    return y_data"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLBucxEyDgyf",
        "colab_type": "text"
      },
      "source": [
        "###### data normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Rk41xrd0XNHq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f4ebfa41-9605-40fd-a915-b24c475eea89"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "dataset_dict = { 0 : 'S_left',        1 : 'S_left_phase',\n",
        "                 2 : 'S_right',       3 : 'S_right_phase',\n",
        "                 4 : 'clean_left',    5 : 'clean_left_phase',\n",
        "                 6 : 'clean_right',   7 : 'clean_right_phase',\n",
        "                 8 : 'idx_drone_end', 9 : 'idx_voice_end',\n",
        "                10 : 'idx_voice_start'}\n",
        "\n",
        "\n",
        "\n",
        "numpy_dict = dict()\n",
        "for n in range(4):\n",
        "    numpy_name    = dataset_dict[n]\n",
        "    numpy_dict[n] = np.load( PATH + numpy_name + '.npy' )\n",
        "    \n",
        "\n",
        "\n",
        "'''    x_data,       y_data '''\n",
        "'''(1000,4,257,382), (1000,)'''\n",
        "\n",
        "x_data = []\n",
        "for idx in range(1000):\n",
        "    x_element = []\n",
        "\n",
        "    for n in range(4):\n",
        "        x_element.append( numpy_dict[n][:,:,idx] )\n",
        "\n",
        "    x_element = np.asarray( x_element )\n",
        "    \n",
        "    #log scale 변환\n",
        "    x_element[0] = np.log10( np.abs(x_element[0]) + np.finfo(np.float32).eps )\n",
        "    x_element[2] = np.log10( np.abs(x_element[2]) + np.finfo(np.float32).eps )\n",
        "\n",
        "\n",
        "    # normalization\n",
        "    x_mean = x_element[0].mean()\n",
        "    x_stdv = x_element[0].std()\n",
        "    x_element[0] = ( x_element[0] - x_mean ) / x_stdv\n",
        "\n",
        "    x_mean = x_element[2].mean()\n",
        "    x_stdv = x_element[2].std()\n",
        "    x_element[2] = ( x_element[2] - x_mean ) / x_stdv\n",
        "\n",
        "\n",
        "    x_data.append( x_element )\n",
        "\n",
        "\n",
        "x_data = np.asarray(x_data)\n",
        "y_data = Y_DATA( np.load(PATH + 'angle.npy') )\n",
        "print('done..')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "colab_type": "code",
        "id": "oH6kABe5XhZT",
        "colab": {}
      },
      "source": [
        "\n",
        "#x_data = x_data.reshape()\n",
        "#y_data = y_data.reshape()\n",
        "\n",
        "\n",
        "\n",
        "x_data = torch.from_numpy( x_data ).float().to('cuda')\n",
        "y_data = torch.from_numpy( y_data ).long().to('cuda')\n",
        "\n",
        "full_dataset = TensorDataset( x_data, y_data )\n",
        "\n",
        "\n",
        "train_dataset, valid_dataset = torch.utils.data.random_split( full_dataset, [train_size, test_size])\n",
        "train_dataset = DataLoader( dataset=train_dataset, batch_size = BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "valid_dataset = DataLoader( dataset=valid_dataset, batch_size = BATCH_SIZE, shuffle=True, drop_last=True)\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PjH37Dc-iHv7"
      },
      "source": [
        "#### ResNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3Qt8_mI1vY0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        \n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)    # 3x3 stride = stride\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)  # 3x3 stride = 1\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x) \n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        \n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        \n",
        "        width = int(planes * (base_width / 64.)) * groups\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        \n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x) # 1x1 stride = 1\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out) # 3x3 stride = stride\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out) # 1x1 planes, plaines * self.expansion\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=11, zero_init_residual=False,\n",
        "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
        "                 norm_layer=None):\n",
        "        \n",
        "        super(ResNet, self).__init__()\n",
        "        \n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        self.inplanes = 64\n",
        "        self.dilation = 1\n",
        "        \n",
        "        if replace_stride_with_dilation is None:\n",
        "            # each element in the tuple indicates if we should replace\n",
        "            # the 2x2 stride with a dilated convolution instead\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        \n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
        "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
        "        \n",
        "        \n",
        "\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "\n",
        "        # input : 3 x 224 x 224\n",
        "        self.conv1 = nn.Conv2d(4, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        \n",
        "        # output = self.conv1( input )\n",
        "        # output : 64 x 112 x 112\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # input : 64 x 112 x 112\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        # output : 64 x 56 x 56\n",
        "\n",
        "        #[layers]\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0])\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1])\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])\n",
        "        \n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
        "                            self.base_width, previous_dilation, norm_layer))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
        "                                base_width=self.base_width, dilation=self.dilation,\n",
        "                                norm_layer=norm_layer))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def _forward_impl(self, x):\n",
        "        # See note [TorchScript super()]\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self._forward_impl(x)\n",
        "\n",
        "\n",
        "\n",
        "def _resnet(arch, block, layers, pretrained, progress, **kwargs):\n",
        "    model = ResNet(block, layers, **kwargs)\n",
        "    if pretrained:\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def resnet50(pretrained=False, progress=True, **kwargs):\n",
        "\n",
        "    return _resnet('resnet50', Bottleneck, [3, 4, 6, 3], pretrained, progress, **kwargs)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8hnA7UlDTeG",
        "colab_type": "text"
      },
      "source": [
        "##### model train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCRi61pl6_pY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "16b12d3e-5dd1-4e9c-e16e-3e64c8665122"
      },
      "source": [
        "torch.manual_seed(1)\n",
        "\n",
        "\n",
        "model = resnet50().to('cuda')\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().to('cuda')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
        "\n",
        "train_loss = []\n",
        "train_acc  = []\n",
        "\n",
        "\n",
        "model.train()\n",
        "for epoch in range(10):\n",
        "    print('epoch' + str(epoch+1))\n",
        "    \n",
        "    for i, (data, label) in enumerate(train_dataset):\n",
        "        (data, label) = (data.to('cuda'), label.to('cuda'))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "  \n",
        "        loss = F.nll_loss(output, label.reshape(BATCH_SIZE))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        preds = output.data.max(1)[1]\n",
        "        corr  = (preds==label.reshape(BATCH_SIZE)).sum().item()\n",
        "        acc   = corr/BATCH_SIZE*100\n",
        "        \n",
        "        train_loss.append(loss.item())\n",
        "        train_acc.append( acc )\n",
        "        \n",
        "        print('\\tLoss: {:.3f}\\tAcc: {:.3f}'.format(loss.item(), acc))\n",
        "        "
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch1\n",
            "\tLoss: -0.188\tAcc: 12.000\n",
            "\tLoss: -0.223\tAcc: 10.000\n",
            "\tLoss: -0.292\tAcc: 12.000\n",
            "\tLoss: -0.317\tAcc: 6.000\n",
            "\tLoss: -0.263\tAcc: 8.000\n",
            "\tLoss: -0.217\tAcc: 6.000\n",
            "\tLoss: -0.273\tAcc: 6.000\n",
            "\tLoss: -0.377\tAcc: 6.000\n",
            "\tLoss: -0.262\tAcc: 2.000\n",
            "\tLoss: -0.359\tAcc: 14.000\n",
            "\tLoss: -0.388\tAcc: 4.000\n",
            "\tLoss: -0.413\tAcc: 8.000\n",
            "\tLoss: -0.442\tAcc: 6.000\n",
            "\tLoss: -0.408\tAcc: 10.000\n",
            "\tLoss: -0.616\tAcc: 10.000\n",
            "\tLoss: -0.503\tAcc: 8.000\n",
            "epoch2\n",
            "\tLoss: -0.695\tAcc: 16.000\n",
            "\tLoss: -0.550\tAcc: 10.000\n",
            "\tLoss: -0.545\tAcc: 6.000\n",
            "\tLoss: -0.560\tAcc: 8.000\n",
            "\tLoss: -0.631\tAcc: 14.000\n",
            "\tLoss: -0.749\tAcc: 12.000\n",
            "\tLoss: -0.715\tAcc: 12.000\n",
            "\tLoss: -0.753\tAcc: 4.000\n",
            "\tLoss: -0.736\tAcc: 4.000\n",
            "\tLoss: -0.851\tAcc: 12.000\n",
            "\tLoss: -0.777\tAcc: 10.000\n",
            "\tLoss: -0.691\tAcc: 6.000\n",
            "\tLoss: -0.707\tAcc: 2.000\n",
            "\tLoss: -0.975\tAcc: 10.000\n",
            "\tLoss: -1.004\tAcc: 14.000\n",
            "\tLoss: -0.978\tAcc: 8.000\n",
            "epoch3\n",
            "\tLoss: -0.865\tAcc: 8.000\n",
            "\tLoss: -0.811\tAcc: 2.000\n",
            "\tLoss: -1.036\tAcc: 12.000\n",
            "\tLoss: -0.959\tAcc: 4.000\n",
            "\tLoss: -1.069\tAcc: 8.000\n",
            "\tLoss: -1.046\tAcc: 10.000\n",
            "\tLoss: -1.175\tAcc: 18.000\n",
            "\tLoss: -1.129\tAcc: 12.000\n",
            "\tLoss: -1.278\tAcc: 12.000\n",
            "\tLoss: -1.232\tAcc: 12.000\n",
            "\tLoss: -1.054\tAcc: 4.000\n",
            "\tLoss: -1.355\tAcc: 20.000\n",
            "\tLoss: -1.279\tAcc: 8.000\n",
            "\tLoss: -1.326\tAcc: 12.000\n",
            "\tLoss: -1.399\tAcc: 12.000\n",
            "\tLoss: -1.268\tAcc: 10.000\n",
            "epoch4\n",
            "\tLoss: -1.308\tAcc: 6.000\n",
            "\tLoss: -1.489\tAcc: 12.000\n",
            "\tLoss: -1.398\tAcc: 12.000\n",
            "\tLoss: -1.529\tAcc: 16.000\n",
            "\tLoss: -1.534\tAcc: 14.000\n",
            "\tLoss: -1.545\tAcc: 18.000\n",
            "\tLoss: -1.499\tAcc: 6.000\n",
            "\tLoss: -1.472\tAcc: 12.000\n",
            "\tLoss: -1.530\tAcc: 8.000\n",
            "\tLoss: -1.690\tAcc: 14.000\n",
            "\tLoss: -1.557\tAcc: 10.000\n",
            "\tLoss: -1.516\tAcc: 8.000\n",
            "\tLoss: -1.833\tAcc: 18.000\n",
            "\tLoss: -1.582\tAcc: 14.000\n",
            "\tLoss: -1.499\tAcc: 2.000\n",
            "\tLoss: -1.797\tAcc: 12.000\n",
            "epoch5\n",
            "\tLoss: -1.869\tAcc: 16.000\n",
            "\tLoss: -1.689\tAcc: 8.000\n",
            "\tLoss: -1.932\tAcc: 14.000\n",
            "\tLoss: -1.881\tAcc: 10.000\n",
            "\tLoss: -2.198\tAcc: 22.000\n",
            "\tLoss: -1.818\tAcc: 10.000\n",
            "\tLoss: -1.842\tAcc: 10.000\n",
            "\tLoss: -1.860\tAcc: 6.000\n",
            "\tLoss: -1.996\tAcc: 12.000\n",
            "\tLoss: -1.955\tAcc: 8.000\n",
            "\tLoss: -1.994\tAcc: 8.000\n",
            "\tLoss: -1.996\tAcc: 12.000\n",
            "\tLoss: -2.020\tAcc: 10.000\n",
            "\tLoss: -2.196\tAcc: 10.000\n",
            "\tLoss: -2.159\tAcc: 12.000\n",
            "\tLoss: -2.418\tAcc: 18.000\n",
            "epoch6\n",
            "\tLoss: -2.298\tAcc: 16.000\n",
            "\tLoss: -2.120\tAcc: 8.000\n",
            "\tLoss: -2.208\tAcc: 16.000\n",
            "\tLoss: -2.316\tAcc: 16.000\n",
            "\tLoss: -2.342\tAcc: 12.000\n",
            "\tLoss: -2.380\tAcc: 4.000\n",
            "\tLoss: -2.417\tAcc: 12.000\n",
            "\tLoss: -2.426\tAcc: 10.000\n",
            "\tLoss: -2.382\tAcc: 10.000\n",
            "\tLoss: -2.330\tAcc: 4.000\n",
            "\tLoss: -2.840\tAcc: 14.000\n",
            "\tLoss: -2.396\tAcc: 10.000\n",
            "\tLoss: -2.561\tAcc: 16.000\n",
            "\tLoss: -2.575\tAcc: 12.000\n",
            "\tLoss: -2.759\tAcc: 14.000\n",
            "\tLoss: -2.855\tAcc: 14.000\n",
            "epoch7\n",
            "\tLoss: -2.510\tAcc: 8.000\n",
            "\tLoss: -2.819\tAcc: 14.000\n",
            "\tLoss: -2.882\tAcc: 14.000\n",
            "\tLoss: -2.901\tAcc: 16.000\n",
            "\tLoss: -2.754\tAcc: 6.000\n",
            "\tLoss: -2.928\tAcc: 14.000\n",
            "\tLoss: -3.070\tAcc: 12.000\n",
            "\tLoss: -3.118\tAcc: 20.000\n",
            "\tLoss: -2.847\tAcc: 20.000\n",
            "\tLoss: -2.761\tAcc: 6.000\n",
            "\tLoss: -3.247\tAcc: 20.000\n",
            "\tLoss: -2.903\tAcc: 8.000\n",
            "\tLoss: -3.071\tAcc: 6.000\n",
            "\tLoss: -3.130\tAcc: 6.000\n",
            "\tLoss: -3.122\tAcc: 10.000\n",
            "\tLoss: -3.104\tAcc: 16.000\n",
            "epoch8\n",
            "\tLoss: -3.178\tAcc: 14.000\n",
            "\tLoss: -3.064\tAcc: 12.000\n",
            "\tLoss: -3.474\tAcc: 20.000\n",
            "\tLoss: -2.944\tAcc: 10.000\n",
            "\tLoss: -3.257\tAcc: 12.000\n",
            "\tLoss: -3.240\tAcc: 8.000\n",
            "\tLoss: -3.259\tAcc: 8.000\n",
            "\tLoss: -3.722\tAcc: 16.000\n",
            "\tLoss: -3.495\tAcc: 14.000\n",
            "\tLoss: -3.357\tAcc: 14.000\n",
            "\tLoss: -3.557\tAcc: 16.000\n",
            "\tLoss: -3.890\tAcc: 12.000\n",
            "\tLoss: -3.413\tAcc: 2.000\n",
            "\tLoss: -3.655\tAcc: 8.000\n",
            "\tLoss: -3.802\tAcc: 16.000\n",
            "\tLoss: -4.008\tAcc: 12.000\n",
            "epoch9\n",
            "\tLoss: -3.408\tAcc: 8.000\n",
            "\tLoss: -4.012\tAcc: 20.000\n",
            "\tLoss: -3.644\tAcc: 10.000\n",
            "\tLoss: -3.838\tAcc: 12.000\n",
            "\tLoss: -4.215\tAcc: 14.000\n",
            "\tLoss: -3.914\tAcc: 8.000\n",
            "\tLoss: -4.107\tAcc: 12.000\n",
            "\tLoss: -4.127\tAcc: 8.000\n",
            "\tLoss: -4.097\tAcc: 8.000\n",
            "\tLoss: -3.826\tAcc: 10.000\n",
            "\tLoss: -4.054\tAcc: 10.000\n",
            "\tLoss: -4.333\tAcc: 16.000\n",
            "\tLoss: -4.276\tAcc: 18.000\n",
            "\tLoss: -4.382\tAcc: 12.000\n",
            "\tLoss: -3.965\tAcc: 8.000\n",
            "\tLoss: -4.304\tAcc: 10.000\n",
            "epoch10\n",
            "\tLoss: -4.021\tAcc: 10.000\n",
            "\tLoss: -4.776\tAcc: 14.000\n",
            "\tLoss: -4.621\tAcc: 14.000\n",
            "\tLoss: -4.134\tAcc: 8.000\n",
            "\tLoss: -4.603\tAcc: 20.000\n",
            "\tLoss: -4.248\tAcc: 14.000\n",
            "\tLoss: -4.359\tAcc: 2.000\n",
            "\tLoss: -4.099\tAcc: 12.000\n",
            "\tLoss: -4.719\tAcc: 10.000\n",
            "\tLoss: -4.894\tAcc: 12.000\n",
            "\tLoss: -4.289\tAcc: 8.000\n",
            "\tLoss: -5.253\tAcc: 28.000\n",
            "\tLoss: -4.886\tAcc: 12.000\n",
            "\tLoss: -4.426\tAcc: 10.000\n",
            "\tLoss: -5.202\tAcc: 14.000\n",
            "\tLoss: -4.285\tAcc: 8.000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lyO8F3xwaGd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}